I. There are three types of factors the author was trying to assess the performance 
of D&R R-Hadoop (the R and Hadoop Integrated Programming Environment).
1. Data factors
1.1 r is the number of subset, 
1.2 v is the number of variables, 
1.3 m is the number of rows per subset. 
So if the total number of observations is n, then n/m=r. s=8mv is the size of subset in 
bytes, S=8rmv is the size of total dataset in bytes.

2. Hadoop HDFS factors
   	2.1 BLK, the size of a block in HDFS, is the independent stored block-sized chunks 
units of datasets in HDFS. When a dataset is used as input to a MapReduce job, use map function
to simulate data does not belong to this situation, the Hadoop scheduler assigns a Map 
task to each block. So there are as many Map tasks as the number of the number of Blocks 
the dataset spans. Please check the example code named "lapply.size.buff.R". The default value
for BLK is 128MB. Given S, the smaller BLK, the larger number of blocks and consequently the 
larger number of Map tasks, and this will creates addtional burden to the NameNode on master
node for file management and also creates additional overhead to the JobTracker on master node
for task management.
	2.2 REP, the number of replications, is a small number of physically separate nodes to 
provide fault tolerance and availability. affects data locality optimization. Hadoop does its 
best to run a Map task on a node where the input data resides in order to avoid remote transfer
of data through the network. However, in Reduce task, remote transfer is necessary, because the
output from each Map will be partitioned to different parts, and then one reducer will grab the 
corresponding part cross all map output files. The number of parts will be equal to the number 
of Reduce tasks. The default value for REP is 3.
	2.3 IOB, Hadoop I/O buffer size, also affect HDFS I/O performance. IOB determines how 
much data is buffered in I/O pipes before transferring to other operations during read and write 
operations. Please check the Streaming section in the book of Hadoop. The default value for IOB
is 4KB.

3. Hadoop MapReduce factors
	3.1 MTC, the Map task capacity, controls the maximum number of Map tasks, which are run
simultaneously on a given node. Collectively, on a cluster with nn number of nodes, at most MTC*nn
Map tasks can be run at any one time. The Map tasks running on the same node will compete with each 
other. Thus too many Map tasks running simultaneously might cause congestion and hurt the performance 
instead. The default value for MTC is 2.
	3.2 RTC, the same concern regarding the MTC also applies to RTC.
	3.3 REUSE, reused of Java Virtual Machine, is how many tasks will be launched in one JVM. 
When a node runs Map or Reduce tasks, it runs the tasks in their own (one or multiple tasks per JVM) 
JVM to isolate them from other running tasks. It takes around a second to start a new JVM for each 
task. The default value for REUSE is 1, which JVM reuse is disabled. -1 means no limit.

###################################################################################################

II. Unique features in terms of map tasks and reduce tasks for D&R
1. Number of MapReduce tasks: there are many more map tasks than reduce tasks. so factors which affect
MapReduce job's I/O, CPU, and memory usagem especially on the map side, are likely to have an impact 
on the performance of D&R computations.

2. I/O: the size of input to map tasks is much larger than the size of intermediate files that are
produced by map tasks, and the size of output generated by reduce tasks.

3. CPU and memory usage: map tasks tend to be more CPU and memeory intensive than reduce task.

####################################################################################################

III. Modeling
1. Categorical Modeling
O is the time to read into memory the subsets from disk where they are stored as R objects, list.
L is the elapsed time of the glm.fit logistic fitting, and T = L + O.
O_ijk = g_0(r_ij, v_ij) + eps1_ijk
T_ijk = g_0(r_ij, v_ij) + g_l(r_ij, v_ij) + eps2_ijk
The assumpton that the dependence of O on the factors is the same in both equations is assured by the 
definition of O. g_0 and g_l each have main effects and there is a full interaction which means
hat O = g_0 = mu_0 + r_i + v_j + rv_ij, 
hat L = hat T - hat O = g_l = mu_l + r_i + v_j + rv_ij

2. Polynomical Modeling
g_0(m, v) = mu_0 + alpha_01*v + beta_01*m
g_l(m, v) = mu_l + alpha_l1*v + alpha_l2*v^2 + beta_l1*m + beta_l2*m^2 + gamma_l*v*m
Two types of residuals
The residual for assessing the fit = log(fitted value in the categorical model) - log(fitted value in the quadratic model)
The redidual for distributional property = log(observed value) - log(fitted value in the quadratic model)


