###########################################
##Install Rhipe on local single core machine
#install javajdk7, java library will be located in /usr/lib/jvm
sudo apt-get install openjdk-7-jdk
#if multiple java version is available, managing java and java compiler as following
sudo update-alternatives --config java
sudo update-alternatives --config javac
#set Java configuration in .bashrc
export JAVA_LD_LIBRARY_PATH=/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/amd64/server
export JAVA=/usr/lib/jvm/java-1.7.0-openjdk-amd64/jre/bin/java
export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-amd64
export JAVAC=/usr/lib/jvm/java-1.7.0-openjdk-amd64/bin/javac
export JAVAH=/usr/lib/jvm/java-1.7.0-openjdk-amd64/bin/javah
export JAVA_CPPFLAGS="-I/usr/lib/jvm/java-1.7.0-openjdk-amd64/include"
export JAVA_LIBS="-L/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/amd64/server -ljvm"
export JAR=/usr/lib/jvm/java-1.7.0-openjdk-amd64/bin/jar


#####################
#install protobuffer
#set a link of protobuf for R. This is building link of protobuf shared 
#lib files to the searching path where R search for shared lib
sudo ln -s protobuf-2.4.1/lib/lib* /usr/local/lib/ 
#install Rhipe package
R CMD INSTALL Rhipe/Rhipe_0.74.0.tar.gz


###########################
##Access jobtracker on adhara
ssh -L50030:adhara1:50030 shaula.stat.purdue.edu
http://localhost:50030


################
##Access jobtracker on rossmann
ssh -L50030:hadoop-01:50030 rossmann-wsc.rcac.purdue.edu
http://localhost:50030


##################
##What is RhipeLib.tar.gz on HDFS?
It is an R distribution loaded with every shared lib file I could find via code that 
a package R has installed might have used. Rhipe itself is basically a Java program 
that calls a C program that is an R interpreter that loads R packages which all have 
random dependencies that must locatable during a Hadoop run. Hadoop itself has a concept 
of a shared cache file that is expected to be a tar.gz, and Rhipe uses that shared cache 
capability.


#################
##RHIPE Options
rhipe_stream_buffer
  The size of the STDIN buffer used to write data to the R process(in bytes) 
  default: 10*1024 bytes
mapred.textoutputformat.separator
  The text that seperates the key from value when inout[2] equals text. default: Tab
mapred.field.separator
  The text that seperates fields when inout[2] equals text. default: Space
rhipe_reduce_buff_size
  The maximum length of reduce.values default: 10,000
rhipe_map_buff_size
  The maximum length of map.values (and map.keys) default: 10,000


##################
##get the file name being processed
Sys.getenv("mapred.input.file")


#####################
##Why RHIPE can write any sized object but cannot read key,values 
##that are more than 256MB?
1.Here 256Mb is the limit for a key/value pair that has already been on HDFS and
will be an input sequence file to another mapreduce job. When we created
this key/value pair as sequence file, we can create(write) any size. But
when we try to use(read) it as input sequence file, we will get error like:
"Error: PB ERROR[LOGLEVEL_ERROR](google/protobuf/io/coded_stream.cc:156) 
A protocol message was rejected because it was too big (more than 268435456 bytes).  
To increase the limit (or to disable these warnings), 
see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h."
2.There is another type of read limit is the limit of key/value pair size when
we call rhread(). So far I have tested on adhara, when one key/value pair is 128MB,
we will get error:
Error in .jcall("RJavaTools", "Ljava/lang/Object;", "invokeMethod", cl,  : 
  java.lang.OutOfMemoryError: Java heap space
The way to solve this issue is call:options(java.parameters = "-Xmx1000m") before
call library(Rhipe)


#####################
##Block size for sequence files
Btw, if you have sequence files as input you can force the map tasks to be large,
by setting the block size to be small e.g. 
rhwatch(..., mapred=list(mapred.max.split.size = numInBytes)) ...


##########
##rhclean
Each time we run a map-reduce job, a temporary is created under /tmp on the 
HDFS. These tmp files are not deleted unless you call rhclean() with default 
argument. It will try to delete all the tmp files created. However, the 
permission setup on our cluster is such that a user is not allowed to delete 
another user's file/directory. We can create a temp folder in your user folder 
and set HADOOP.TMP.FOLDER to that.


####################
##Why we will have tmp file for each map-reduce job
temp files are created if params are used, or if references to globals values 
are detected. temp files are also created if output is missing (a temp output 
folder is created).


##################
##What is the difference between mappers/reducers and maptasks/reducetasks?
mappers/reducers have no clear defination, but they usually refers to the 
core that are running map tasks or reduce tasks.


###################
##What is map/reduce task capacity?
The number of tasks can be executed at the one moment. For instance, on adhere
the map/reduce task capacity is 72. This can be set by "mapred.tasktracker.map.tasks.maximum".


######
##Map
The map is an R expression (created using the R command expression) that is evaluated by 
RHIPE during the map stage. For each task, RHIPE will call this expression multiple times. 
If a task consists of W key,value pairs, the expression map will be called 
\(\lceil \frac{W}{ \text{rhipe\_map\_buffsize}} \rceil\) times. The default value of 
rhipe_map_buff_size is 3,000 and is user configurable. Each time map is called, the vectors 
map.keys and map.values contain rhipe_map_buff_size keys and values respectively. If the objects
are large it advisable to reduce the size of rhipe_map_buff_size . See the Airline examples where
the value was set to 10 (each value was 5000x8 data frame).


###########
##Reduce
Each Reduce task is a partition of the intermediate keys produced as the output of the Map phase. 
The above code is run for every Reduce task. RHIPE implements the above algorithm by calling the 
R expression reduce$pre at line 3. In this expression, the user will have the new key present in 
reduce.key. After which RHIPE will call reduce$reduce several times until the condition in line 4 
is false. Each time reduce$reduce is called, the vector reduce.values will contain a subset of 
the intermediate map values associated with reduce.key. The length of this vector is a default 
10,000 but can be changed via the rhipe_reduce_buff_size option. Finally when all values have been
processed, RHIPE calls reduce$post at line 7. At this stage, all intermediate values have been sent 
and the user is expected to write out the final results. Variables created in reduce$pre will be 
visible in the subsequent expressions. Thus to compute the sum of all the intermediate values,
reduce is optional, and if not specified the map output keys will be sorted and shuffled and saved 
to disk. Thus it is possible to set inout[2] to map when the reduce expression is not given to obtain 
a MapFile. To turn of sorting and shuffling and instead write the map output to disk directly, 
set mapred.reduce.tasks to zero in mapred. In this case, the output keys are not sorted and the output
format should not be map


################
##What determines how many maptasks/reducetasks are run?
For input like "sequence" and "text", the number of maptasks is determined 
by the number of blocks of the data. The default size of one block in HDFS 
is 128MB. For lapply input, which means input=c(N,maptasks), we can specify 
how many maptasks are run in input arguement. Here is a trick, we definitely 
can specify any number of maptasks in lapply input, however, the blocks 
effect still exist. For instance, if we are planning to create a data with 
1GB, each subset is 16MB, so totally we have 2^6 subset. If we specify the 
input=c(2^6, 6), there will be just 6 maptasks, and we specify the # of reduce
to anything we like, e.g. 1. However, if we use the output of this map only 
mapreduce job to be an input for another mapreduce job, the number of maptasks 
will be 1GB/128MB=8 instead of 1. This means even though we only have 1 partition
from the first job, there is actually 8 maptasks for second job. On the other 
hand, if we specify the input=c(2^6, 6), and mapred.reduce.tasks=10, which means 
we specify more maptasks number than it need, same as 1, there will be 8 maptasks
for furture mapreduce.


##################
##What is syc points?
The block size of the files in HDFS is 128MB. The HDFS will cut the files to 
blocks based on this size, like put a bar right at the 128MB end. At the same
time, there is another small bar called syc point. HDFS put this syc points
every around 4KB in the files. So the first maptask will get the first block
plus the extra until the first syc point in the second block. And the second
maptask will get the second block but only work from the first syc point, and
also get the extra until the first syc point in the third block. There is a 
special situation. We know that the key/value pair is the smallest executived
unit in mapreduce. If the last key/value pair takes 2MB space in the second 
block, the first maptask will read the first block and the extra in the second
block until the last key/value pair end. So the cut off point for each maptask
is like min(first syc point, structure bound). 


#########################
##How are KV pairs assigned to mappers/reducers?
The map argument is an R expression. Hadoop will read key,value pairs, send 
them to RHIPE which in turn buffers them by storing them in a R list: map.values 
and map.keys respectively. Once the buffer is full, RHIPE calls the map expression. 
The default length of map.values (and map.keys) is 10,000


########################
##Number of tasks per node
"mapred.tasktracker.map.tasks.maximum" deals with the number of map tasks 
that should be launched on each node, not the number of nodes to be used 
for each map task. In the Hadoop architecture, there is 1 tasktracker for 
each node (slaves) and 1 job tracker on a master node (master). So if you 
set the property mapred.tasktracker.map.tasks.maximum, it will only change 
the number of map tasks to be executed per node. The range of 
"mapred.tasktracker.map.tasks.maximum" is from 1/2*cores/node to 2*cores/node.


#########################
##Is it possible that multiple nodes working on the same task?
It should be asked that is it possible that multiple cores working on the same
task. Task is the small unit for a Mapreduce to execute. If the task is above
128MB, that task will be divided to two tasks. If the task is smaller than 128MB
then it will be distributed to a core to be executed. However, if the number of
key/value pairs are more than 3000, then the core will run the part of task each
time. For example, the task has 4000 key/value pairs, then the core will first
run the first 3000 pairs, then same core will run the 1000 left pairs. So the 
core is executing the task iteratively. The answer to the question is yes. Since
the task is the smallest unit for execution, then we cannot use two cores to
execute the task in such way that we combine the computation ability of two cores
to execute that task. However, two cores work on one task means two cores will
execute the task independently. If the original core which are running the task
is very slow, and there is another core is avaliable, then the second core will
start to execute the task, whichever finish first, the other one will be killed,
all temporary files will be deleted.


#############################
##What is the difference between job and task?
A mapreduce code together is called a job. Jobtracker will give us an idea that
how many percent of the job has been finished.
One the worker nodes, besides the cores that will be used to execute mapreduce
task, there are other two cores to be used to process tasktracker and datanode.
For master node, besides the cores that will be used to execute mapreduce
task, there are other four cores to be used to process tasktracker, jobtracker,
datanode, and namenode. Mapreduce and HDFS are two relatively independent two
systems, but do exist comunication between these two systems. Namenode and
datanode are from HDFS aspect, tasktracker and jobtracker are from mapreduce
aspect. 


#########################
##What is jobtracker?
The JobTracker is the service within Hadoop that farms out MapReduce tasks 
to specific nodes in the cluster, ideally the nodes that have the data, or at 
least are in the same rack. 
1. Client applications submit jobs to the Jobtracker.
2. The JobTracker talks to the NameNode to determine the location of the data
3. The JobTracker locates TaskTracker nodes with available slots at or near the data
4. The JobTracker submits the work to the chosen TaskTracker nodes.
5. The TaskTracker nodes are monitored. If they do not submit heartbeat signals 
   often enough, they are deemed to have failed and the work is scheduled on a 
   different TaskTracker.
6. A TaskTracker will notify the JobTracker when a task fails. The JobTracker 
   decides what to do then: it may resubmit the job elsewhere, it may mark that 
   specific record as something to avoid, and it may may even blacklist the TaskTracker 
   as unreliable.
7. When the work is completed, the JobTracker updates its status.
8. Client applications can poll the JobTracker for information. 


#####################
##What is tasktracker?
A TaskTracker is a node in the cluster that accepts tasks - Map, Reduce and Shuffle 
operations - from a JobTracker.
Every TaskTracker is configured with a set of slots, these indicate the number of 
tasks that it can accept. When the JobTracker tries to find somewhere to schedule a 
task within the MapReduce operations, it first looks for an empty slot on the same 
node that hosts the DataNode containing the data, and if not, it looks for an empty 
slot on a machine in the same rack.
The TaskTracker spawns a separate JVM processes to do the actual work; this is to 
ensure that process failure does not take down the task tracker. The TaskTracker 
monitors these spawned processes, capturing the output and exit codes. When the process 
finishes, successfully or not, the tracker notifies the JobTracker. The TaskTrackers 
also send out heartbeat messages to the JobTracker, usually every few minutes, to 
reassure the JobTracker that it is still alive. These message also inform the JobTracker 
of the number of available slots, so the JobTracker can stay up to date with where in 
the cluster work can be delegated. 


#################################
##What if there is a large number of values associated with one key?
In the RHIPE, there is a limit for number of key/value pairs belong to one
map.keys/map.values pair. This means the buff size for map.keys/map.values
pair is 3000 key/value pairs per map.keys/map.values pair. For both "lapply"
and "sequence" input, this count buff effect exist. The map.tasks number
here will not affect this 3000 buff. For instance, input=c(9003,3), the task 
number is 3, but the number of map.keys/map.values pairs are 6, which means
each task will handle two map.keys/values packages. Another situation is for
"sequence" input, if more than 3000 values associated with one key, those
key/value pairs will be distributed to different map.keys/map.values pairs. 
Here the reduce.tasks only controls how many pairtitions there will be for
the intermediate output. Here map.keys/map.values pairs tend to be equally
distributed to pairtitions. I think after map function, all key/value pairs
are tend to be equally distributed to pairtitions. If the number of reduce.tasks
is larger than the key/value pairs, then some of partitions will be empty 
files.
Same count buff effect exist for reduce.keys/reduce.values pairs. If more
than 3000 key/value pairs are related to one key, they will be distributed
to different reduce.keys/reduce.values pairs. This is why we have to use
pre, reduce, and post to combine results for same key from different 
reduce.keys/reduce.values pairs.


###########################
##Is map.keys/map.values the package worked on one node or on one core?
On one core for sure. map.keys/map.values are the execution objects for one core.
If the number of key/value pairs are smaller than 3000 in that map.keys/map.values
package, then this map.keys/map.values pair will be executed on the core at one time.
On the other hand, if the number of key/value pairs are more than 3000 in that 
map.keys/map.values pair, then those key/value pairs will be executed on the same 
core at different execution iterative time.


####################
## What is namenode?
THe namenode manages the filesystem namespace. It maintains the filesystem tree
and the metadata for all the files and directories in the tree. THis information is
stored persistently on local disk in the form of two files: namespace image and edit
log. Namenode also knows the datanodes on which all the blocks for a given file are
located, but it does not store block locations persistently.
Under federation, each namenode manage a namespace volume, which is made up of the 
metadata for the namespace and a block pool containing all the blocks for the files
in the namespace. Namespace volumes are independent of each other. However datanodes
register with each namenode in the cluster and store blocks from multiple block pools.


#####################
## What is datanode?
Datanodes are the workhorses of the HDFS. The store and retrieve blocks when they
are told, and they report back to the namenode periodically with lists of blocks
that they are storing.


##########################
## What is secondary namenode?
The namenode stores the HDFS filesystem information in a file named fsimage. 
Updates to the file system (add/remove blocks) are not updating the fsimage file, 
but instead are logged into a file, so the I/O is fast append only streaming as 
opposed to random file writes. When restaring, the namenode reads the fsimage 
and then applies all the changes from the log file to bring the filesystem 
state up to date in memory. This process takes time.
The secondarynamenode job is not to be a secondary to the name node, but only to 
periodically read the filesystem changes log and apply them into the fsimage file, 
thus bringing it up to date. This allows the namenode to start up faster next time. 


##################################
## How file read happened on HDFS
The client call open() on FileSystem object, which for HDFS is an instance of 
DistributedFileSystem. This DistributedFileSystem object will call the namenode.
For each block the namenode returns the addresses of the datanodes that have a copy 
of that block. Furthermore, the datanodes are sorted according to their distance to 
the client. DistributedFileSystem will return an FSDataInputStream object to the 
client for it to read data from. FSDataInputStream in turn creates a DFSInputStream
object.

Then client will call read(), DFSInputStream, which stored the datanode addresses,
connects to the first datanode for the first block. When the end of the block is
reached, DFSInputStream will close the connection to the datanode, then find the next
best datanode for the next block. DFSInputStream also will call the namenode to 
retrieve the datanode locations for the next batch of blocks as needed.


##################################
## How file write happened on HDFS
THe client call create() on DistributedFileSystem object, DistributedFileSystem then call
the namenode to create a new file in the filesystem's namespace with no blocks associated 
with it. DistributedFileSystem will return an FSDataOutputStream, then it will in turn
creates a DFSOutputStream object.

DFSOutputStream splits the wroten data into packets, and writes them to data queue. The
data queue is consumed by the DataStreamer which is responsible for asking the namenode
to allocate new blocks by pocking a list of suitale datanodes to store the replicas.
The list of datanodes forms a pipeline. The DataStreamer streams the packets to the first
datanode in the pipeline, which stores the packet and forwards it to the second datanode
in the pipeline. DFSOutputStream also maintains a ask queue of packets that are waiting 
to be acknowledged by datanodes. A packet is removed from the ask queue only when it has
been wroten to all datanodes in the pipeline.


##################
## Shuffle and sort
Each map task has a circular memory buffer that it writes the output to. The buffer
is 100 Mb by default, a size that can be tuned by changing the io.sort.mb property.
When contents of the buffer reaches a certain threshold size (io.osrt.spill.percent,
which has the default 0.80), a background thread will start to SPILL the contents to 
disk. Map outputs will continue to be written to the buffer while the spill taskes
place, but if the buffer fills up during this time, the map will block util the spill
is complete.

SPILL are written to the directories specified by the mapred.local.dir property.

Before SPILL writes to disk, the thread first divides the data into partitions corresponding
to the reducers. Within each partition, the background thread performs an in-memory
sort by key, and if there is a combiner function, it is run on the output of the sort.

Each time the memor buffer reaches the spill threshold, a new spill file is created,
so there could be several spill files. Each spill files have same partitions number
and each partition has been sorted. Before the task is finished, the spill files are
merged into a single partitioned and sorted output file.

If there are at least three spill files, the combiner is run again before the output 
file is written. If there are only one or two spill files, no need for the overhead
in invoking the combiner.

It is often good to compress the map output as it is written to disk. By default, the
output is not compressed, but it is easy to enable this by mapred.compress.map.output 
to be TRUE.

The reduce task needs the map output for its particular partition from several map tasks
across the cluster. The map tasks may finish at different times, so the reduce task starts
copying their outputs as soon as each completes, this known as the [COPY PHASE] of the reduce
task.

The map outputs are copied to the reduce task JVM's memory if therea re small enough,
otherwise, they are copied to disk of tasktracker of reduce. When the in-memory buffer
reaches a threshold size or reaches a threshold number of map outputs, it is merged and spilled
to disk of tasktracker of reduce.

When all the map outputs have been copied, the reduce task moves into the [SORT PHASE]
which should properly be called the [MERGE PHASE] as the sorting was carried out on the
map side. This phase merges the the same partitions from different map task outputs, maintaining
their sort ordering. This is done in rounds, which is also true in RHIPE, that the 
reduce-reduce will be run multiple times.

The output of reduce phase is written diretly to the HDFS. Because the tasktracker node is 
also running a datanode, the first block replica will be written to the local disk.

The default mapreduce job will sort input records by their keys, which called partial sort.
If we want to produce a globally sorted file, the naive ansewer is to use a single partition.
But this is too inefficient. Instead, we can use a partitioner that respects the total order.
For example, if we have four partitions, we could put keys for less than -10 in the first 
partition, those -10 < keys < 0 in the second, those 0 < keys < 10 in the rhird, and over 10
in the fourth. But we have to choose the partition sizes carefully to ensure that they are 
fairly even. It is possible to get a fairly even set of partitions by sampling the key 
space. The idea behind sampling is that we look at a small subset of the keys to approximate 
the key distribution. Hadoop first have InputSampler class which will return a sample of keys,
then writePartitionFile() method on InputSampler is used to creates a sequence file to store
the keys that define the partitions.


####################
## Secondary Sort
If the key has multiple components, like IP address: (192.168.1.1) ==> c(192, 168, 1, 1)
We need two things to make the secondary sort work. First we need a new custemized partitioner.
By seeting apartitioner to partition by the specific fields of the key, we can guarantee that
key-value pairs for the same field of key will go to the same reducer.

Another thing is a grouping comparator, it sorts the keys. We can specify different order patern
for different fields of the key.

